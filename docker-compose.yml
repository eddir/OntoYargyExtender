version: "3"

services:
  pg:
    restart: always
    image: postgres:11
    volumes:
      - pgdata_ontologyfiller:/var/lib/postgresql/data/
    environment:
      POSTGRES_DB: oe
      POSTGRES_USER: oe
      POSTGRES_NAME: oe
      POSTGRES_PASSWORD: oepass
    expose:
      - "5432"
  pgadmin:
    container_name: pgadmin
    restart: always
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: "ea@rostkov.me"
      PGADMIN_DEFAULT_PASSWORD: "oepass"
    #    volumes:
    #     - ./docker/pgdmin/servers.json:/pgadmin4/servers.json # preconfigured servers/connections
    #     - ./docker/pgdmin/pgpass:/pgpass # passwords for the connections in this file
    ports:
      - "8008:80"
    networks:
      - default
    depends_on:
      - pg

  django:
    restart: always
    build:
      context: ./
      dockerfile: ./docker/django/Dockerfile
    volumes:
      - ./backend:/app/backend
    depends_on:
      - pg
    networks:
      - broker-kafka
      - default
    environment:
      POSTGRES_DB: oe
      POSTGRES_USER: oe
      POSTGRES_NAME: oe
      POSTGRES_PASSWORD: oepass
      POSTGRES_HOST: "pg"
      POSTGRES_PORT: "5432"
      APP_DOMAIN: ${APP_DOMAIN}
      SECRET_KEY: ${SECRET_KEY}
      KAFKA_HOST: kafka:29092
      KAFKA_TOPIC: "fill_ontology"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_STORAGE_BUCKET_NAME: ${AWS_STORAGE_BUCKET_NAME}
      AWS_S3_REGION_NAME: ${AWS_S3_REGION_NAME}
      AWS_S3_ENDPOINT_URL: ${AWS_S3_ENDPOINT_URL}
    command: >
      sh -c "pipenv run python manage.py makemigrations && 
      pipenv run python manage.py migrate auth && 
      pipenv run python manage.py migrate authentication && 
      pipenv run python manage.py migrate && 
      pipenv run python manage.py shell -c \"from django.contrib.auth.models import User; User.objects.filter(username='admin').exists() or User.objects.create_superuser('admin', 'ea@rostkov.me', 'admin')\" &&
      pipenv run python manage.py collectstatic --noinput && 
      pipenv run gunicorn --bind :8000 --workers=5 --threads=2 --timeout 600 OntoMuseum.wsgi:application"

  nginx:
    restart: always
    build:
      context: ./
      dockerfile: docker/nginx/Dockerfile
    ports:
      - "8888:80"
    networks:
      - default
      - broker-kafka
    volumes:
      - ./backend:/app/backend
      - ./docker/nginx/nginx.conf:/etc/nginx/conf.d/nginx.conf
    depends_on:
      - django

  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    container_name: zookeeper
    networks:
      - broker-kafka
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
  kafka:
    image: confluentinc/cp-kafka:6.2.0
    container_name: kafka
    networks:
      - broker-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ADVERTISED_HOST_NAME: kafka:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      # KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      # KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      # KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
  kafdrop:
    image: obsidiandynamics/kafdrop:3.27.0
    networks:
      - broker-kafka
    depends_on:
      - kafka
      - zookeeper
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:29092

  ontology-extender:
    build:
      context: ./ontologyExtender
      dockerfile: Dockerfile
    environment:
      - KAFKA_INPUT_TOPIC_NAME=fill_ontologies
      - KAFKA_OUTPUT_TOPIC_NAME=ontologies_filled
      - KAFKA_SERVER=kafka
      - KAFKA_PORT=29092
      - PG_HOST=pg
      - PG_PORT=5432
      - PG_DB=oe
      - PG_USER=oe
      - PG_PASSWORD=oepass
      - KAFKA_TOPIC=fill_ontology
      - PUSHER_APP_ID=${PUSHER_APP_ID}
      - PUSHER_APP_KEY=${PUSHER_APP_KEY}
      - PUSHER_APP_SECRET=${PUSHER_APP_SECRET}
      - PUSHER_CLUSTER=${PUSHER_CLUSTER}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_STORAGE_BUCKET_NAME=${AWS_STORAGE_BUCKET_NAME}
      - AWS_S3_REGION_NAME=${AWS_S3_REGION_NAME}
      - AWS_S3_ENDPOINT_URL=${AWS_S3_ENDPOINT_URL}
    restart: "always"
    depends_on:
      - zookeeper
      - kafka
    networks:
      - broker-kafka
      - default

volumes:
  pgdata_ontologyfiller: { }

networks:
  default:
    driver: bridge
  broker-kafka:
    driver: bridge
